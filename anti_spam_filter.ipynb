{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2561be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ab263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the spam dataset...\n",
      "Dataset loaded successfully with 5171 rows and 4 columns.\n",
      "\n",
      "=== DATASET SNAPSHOT ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5171 entries, 0 to 5170\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  5171 non-null   int64 \n",
      " 1   label       5171 non-null   object\n",
      " 2   text        5171 non-null   object\n",
      " 3   label_num   5171 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 161.7+ KB\n",
      "None\n",
      "   Unnamed: 0 label                                               text  \\\n",
      "0         605   ham  Subject: enron methanol ; meter # : 988291\\nth...   \n",
      "1        2349   ham  Subject: hpl nom for january 9 , 2001\\n( see a...   \n",
      "2        3624   ham  Subject: neon retreat\\nho ho ho , we ' re arou...   \n",
      "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
      "4        2030   ham  Subject: re : indian springs\\nthis deal is to ...   \n",
      "\n",
      "   label_num  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          1  \n",
      "4          0  \n",
      "\n",
      "=== DATA DICTIONARY ===\n",
      "              Type  Non-Null Count  Null Count  Null %  Unique Values\n",
      "Unnamed: 0   int64            5171           0     0.0           5171\n",
      "label       object            5171           0     0.0              2\n",
      "text        object            5171           0     0.0           4993\n",
      "label_num    int64            5171           0     0.0              2\n",
      "\n",
      "Detected technical index column 'Unnamed: 0' - will be dropped\n",
      "Column dropped successfully\n",
      "\n",
      "=== LABEL VERIFICATION ===\n",
      "'label' column values:\n",
      "label\n",
      "ham     3672\n",
      "spam    1499\n",
      "Name: count, dtype: int64\n",
      "\n",
      "'label_num' column values:\n",
      "label_num\n",
      "0    3672\n",
      "1    1499\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Consistency check (label vs label_num):\n",
      "label_num     0     1\n",
      "label                \n",
      "ham        3672     0\n",
      "spam          0  1499\n",
      "Spam corresponds to label_num=1: True\n",
      "\n",
      "=== TEXT CONTENT VERIFICATION ===\n",
      "Text length statistics:\n",
      "count     5171.00000\n",
      "mean      1029.74531\n",
      "std       1505.10317\n",
      "min         10.00000\n",
      "25%        238.00000\n",
      "50%        529.00000\n",
      "75%       1214.00000\n",
      "max      31860.00000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Found 0 very short texts (less than 10 characters):\n",
      "\n",
      "Found 0 whitespace-only texts:\n",
      "\n",
      "Texts containing 'Subject:': 5171 out of 5171 (100.00%)\n",
      "\n",
      "Example with 'Subject:':\n",
      "Subject: enron methanol ; meter # : 988291\n",
      "this is a follow up to the note i gave you on monday , 4 / 3 / 00 { preliminary\n",
      "flow data provided by daren } .\n",
      "please override pop ' s daily volume { presen...\n",
      "\n",
      "=== PRELIMINARY FINDINGS ===\n",
      "- Total records: 5171\n",
      "- Missing values: None\n",
      "- Very short texts: 0\n",
      "- Whitespace-only texts: 0\n",
      "- Class distribution: {'ham': 3672, 'spam': 1499}\n",
      "- Spam corresponds to label_num=1: True\n"
     ]
    }
   ],
   "source": [
    "# Step 1 â€” Data intake & Basic EDA (tailored approach)\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading the spam dataset...\")\n",
    "df = pd.read_csv('spam_dataset.csv')\n",
    "print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "# 1) Snapshot & Data Dictionary\n",
    "print(\"\\n=== DATASET SNAPSHOT ===\")\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(\"\\n=== DATA DICTIONARY ===\")\n",
    "# Display data types and null counts\n",
    "data_dict = pd.DataFrame({\n",
    "    'Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null %': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Unique Values': [df[col].nunique() for col in df.columns]\n",
    "})\n",
    "print(data_dict)\n",
    "\n",
    "# Check for 'Unnamed: 0' column (index to drop)\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    print(\"\\nDetected technical index column 'Unnamed: 0' - will be dropped\")\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    print(\"Column dropped successfully\")\n",
    "\n",
    "# Verify label values\n",
    "print(\"\\n=== LABEL VERIFICATION ===\")\n",
    "print(\"'label' column values:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Verify label_num values and consistency with label\n",
    "print(\"\\n'label_num' column values:\")\n",
    "print(df['label_num'].value_counts())\n",
    "\n",
    "# Check consistency between label and label_num\n",
    "if 'label' in df.columns and 'label_num' in df.columns:\n",
    "    # Create a crosstab to verify alignment\n",
    "    label_consistency = pd.crosstab(df['label'], df['label_num'], \n",
    "                                    rownames=['label'], \n",
    "                                    colnames=['label_num'])\n",
    "    print(\"\\nConsistency check (label vs label_num):\")\n",
    "    print(label_consistency)\n",
    "    \n",
    "    # Verify if spam == 1\n",
    "    spam_is_one = (df[df['label'] == 'spam']['label_num'] == 1).all()\n",
    "    print(f\"Spam corresponds to label_num=1: {spam_is_one}\")\n",
    "    \n",
    "    if not spam_is_one:\n",
    "        print(\"WARNING: Inconsistency between 'label' and 'label_num'!\")\n",
    "\n",
    "# Check for empty or very short texts\n",
    "print(\"\\n=== TEXT CONTENT VERIFICATION ===\")\n",
    "# Check text length statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(\"Text length statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Identify very short texts (potential empty content)\n",
    "short_threshold = 10  # Define threshold for \"very short\" texts\n",
    "short_texts = df[df['text_length'] < short_threshold]\n",
    "print(f\"\\nFound {len(short_texts)} very short texts (less than {short_threshold} characters):\")\n",
    "if not short_texts.empty:\n",
    "    print(short_texts[['label', 'text']])\n",
    "\n",
    "# Check for whitespace-only texts\n",
    "whitespace_texts = df[df['text'].str.strip() == '']\n",
    "print(f\"\\nFound {len(whitespace_texts)} whitespace-only texts:\")\n",
    "if not whitespace_texts.empty:\n",
    "    print(whitespace_texts[['label', 'text']])\n",
    "\n",
    "# Verify if texts contain \"Subject:\" pattern\n",
    "has_subject = df['text'].str.contains('Subject:', case=False, regex=True)\n",
    "print(f\"\\nTexts containing 'Subject:': {has_subject.sum()} out of {len(df)} ({has_subject.mean()*100:.2f}%)\")\n",
    "\n",
    "# Display a few examples of texts with and without \"Subject:\"\n",
    "if has_subject.any():\n",
    "    print(\"\\nExample with 'Subject:':\")\n",
    "    print(df[has_subject].iloc[0]['text'][:200] + \"...\")\n",
    "    \n",
    "if (~has_subject).any():\n",
    "    print(\"\\nExample without 'Subject:':\")\n",
    "    print(df[~has_subject].iloc[0]['text'][:200] + \"...\")\n",
    "\n",
    "print(\"\\n=== PRELIMINARY FINDINGS ===\")\n",
    "print(f\"- Total records: {len(df)}\")\n",
    "print(f\"- Missing values: {'None' if df.isnull().sum().sum() == 0 else df.isnull().sum().sum()}\")\n",
    "print(f\"- Very short texts: {len(short_texts)}\")\n",
    "print(f\"- Whitespace-only texts: {len(whitespace_texts)}\")\n",
    "print(f\"- Class distribution: {df['label'].value_counts().to_dict()}\")\n",
    "print(f\"- Spam corresponds to label_num=1: {spam_is_one if 'label' in df.columns and 'label_num' in df.columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf36198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATES ANALYSIS ===\n",
      "Total rows marked as duplicates: 321 (6.21% of dataset)\n",
      "Number of duplicate groups: 143\n",
      "\n",
      "Duplicates by class:\n",
      "- ham: 264 rows (7.19% of ham class)\n",
      "- spam: 57 rows (3.80% of spam class)\n",
      "\n",
      "Top 3 duplicate groups:\n",
      "- 'calpine daily gas nomination...' (20 occurrences)\n",
      "- '...' (16 occurrences)\n",
      "- 'you can be smart !...' (3 occurrences)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€” Duplicates analysis (without removal)\n",
    "\n",
    "# Flag exact duplicates\n",
    "df['is_exact_duplicate'] = df.duplicated('text', keep=False)\n",
    "\n",
    "# Calculate metrics\n",
    "n_rows_dup_marked = df['is_exact_duplicate'].sum()\n",
    "n_groups_dup = df['text'].value_counts().gt(1).sum()\n",
    "\n",
    "# Count duplicates by class\n",
    "dup_by_class = df[df['is_exact_duplicate']].groupby('label').size()\n",
    "dup_pct_by_class = df[df['is_exact_duplicate']].groupby('label').size() / df.groupby('label').size() * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"=== DUPLICATES ANALYSIS ===\")\n",
    "print(f\"Total rows marked as duplicates: {n_rows_dup_marked} ({n_rows_dup_marked/len(df)*100:.2f}% of dataset)\")\n",
    "print(f\"Number of duplicate groups: {n_groups_dup}\")\n",
    "print(\"\\nDuplicates by class:\")\n",
    "for label, count in dup_by_class.items():\n",
    "    print(f\"- {label}: {count} rows ({dup_pct_by_class[label]:.2f}% of {label} class)\")\n",
    "\n",
    "# Show top 3 duplicate groups\n",
    "print(\"\\nTop 3 duplicate groups:\")\n",
    "top_dups = df['text'].value_counts().nlargest(3)\n",
    "for text, count in top_dups.items():\n",
    "    # Extract subject or first 80 chars for display\n",
    "    subject_match = re.search(r'(?im)^\\s*subject\\s*:\\s*(.*)$', text)\n",
    "    display_text = subject_match.group(1) if subject_match else text[:80]\n",
    "    print(f\"- '{display_text}...' ({count} occurrences)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06910a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting subject and body...\n",
      "\n",
      "=== SUBJECT/BODY STATISTICS BY CLASS ===\n",
      "  Class               Metric                      IQR\n",
      "0  SPAM       subject_length      32.00 [20.00-46.00]\n",
      "1  SPAM          body_length  507.00 [227.50-1171.00]\n",
      "2  SPAM  subject_token_count        7.00 [4.00-10.00]\n",
      "3  SPAM     body_token_count    105.00 [46.00-232.00]\n",
      "4  SPAM   subject_body_ratio         0.06 [0.02-0.15]\n",
      "5   HAM       subject_length      28.00 [19.00-40.00]\n",
      "6   HAM          body_length  473.00 [190.00-1170.25]\n",
      "7   HAM  subject_token_count         6.00 [4.00-9.00]\n",
      "8   HAM     body_token_count    118.00 [44.00-284.00]\n",
      "9   HAM   subject_body_ratio         0.05 [0.02-0.15]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” Parsing subject / body\n",
    "\n",
    "# Define robust extraction functions\n",
    "def extract_subject(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    match = re.search(r'(?im)^\\s*subject\\s*:\\s*(.*)$', text)\n",
    "    return match.group(1).strip() if match else ''\n",
    "\n",
    "def extract_body(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    match = re.search(r'(?im)^\\s*subject\\s*:\\s*(.*)$', text)\n",
    "    if match:\n",
    "        # Find the position after the subject line\n",
    "        subject_end = match.end()\n",
    "        # Find the next line break after subject\n",
    "        next_line = text.find('\\n', subject_end)\n",
    "        if next_line != -1:\n",
    "            return text[next_line+1:].strip()\n",
    "        else:\n",
    "            return ''  # No body after subject\n",
    "    return text  # If no subject found, return full text\n",
    "\n",
    "# Add columns\n",
    "print(\"Extracting subject and body...\")\n",
    "df['subject'] = df['text'].apply(extract_subject)\n",
    "df['body'] = df['text'].apply(extract_body)\n",
    "\n",
    "# Calculate length and token metrics\n",
    "df['subject_length'] = df['subject'].fillna('').str.len()\n",
    "df['body_length'] = df['body'].fillna('').str.len()\n",
    "df['subject_token_count'] = df['subject'].fillna('').str.split().str.len()\n",
    "df['body_token_count'] = df['body'].fillna('').str.split().str.len()\n",
    "df['subject_body_ratio'] = df['subject_length'] / df['body_length'].apply(lambda x: max(1, x))\n",
    "\n",
    "# Print statistics by class\n",
    "print(\"\\n=== SUBJECT/BODY STATISTICS BY CLASS ===\")\n",
    "\n",
    "metrics = ['subject_length', 'body_length', 'subject_token_count', 'body_token_count', 'subject_body_ratio']\n",
    "\n",
    "# Create a results dataframe for better display\n",
    "results = []\n",
    "\n",
    "for label_num in [1, 0]:  # SPAM=1, HAM=0\n",
    "    class_label = 'SPAM' if label_num == 1 else 'HAM'\n",
    "    class_df = df[df['label_num'] == label_num]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        median = class_df[metric].median()\n",
    "        q1 = class_df[metric].quantile(0.25)\n",
    "        q3 = class_df[metric].quantile(0.75)\n",
    "        \n",
    "        results.append({\n",
    "            'Class': class_label,\n",
    "            'Metric': metric,\n",
    "            'Median': median,\n",
    "            'Q1': q1,\n",
    "            'Q3': q3,\n",
    "            'IQR': f\"{median:.2f} [{q1:.2f}-{q3:.2f}]\"\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "stats_df = pd.DataFrame(results)\n",
    "print(stats_df[['Class', 'Metric', 'IQR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5e2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating subject indicators...\n",
      "\n",
      "=== SUBJECT INDICATORS BY CLASS (mediana [IQR]) ===\n",
      "   Class                          Indicator    Median [Q1-Q3]\n",
      "0   SPAM                     subject_has_re  0.00 [0.00-0.00]\n",
      "1   SPAM                    subject_has_fwd  0.00 [0.00-0.00]\n",
      "2   SPAM          subject_exclamation_count  0.00 [0.00-0.00]\n",
      "3   SPAM                subject_digit_count  0.00 [0.00-0.00]\n",
      "4   SPAM               subject_has_currency  0.00 [0.00-0.00]\n",
      "5   SPAM  subject_percent_uppercase_letters  0.00 [0.00-0.00]\n",
      "6    HAM                     subject_has_re  0.00 [0.00-0.00]\n",
      "7    HAM                    subject_has_fwd  0.00 [0.00-0.00]\n",
      "8    HAM          subject_exclamation_count  0.00 [0.00-0.00]\n",
      "9    HAM                subject_digit_count  1.00 [0.00-5.00]\n",
      "10   HAM               subject_has_currency  0.00 [0.00-0.00]\n",
      "11   HAM  subject_percent_uppercase_letters  0.00 [0.00-0.00]\n",
      "\n",
      "=== SUBJECT FLAGS PREVALENCE (percentuale di True) ===\n",
      "Class                   HAM  SPAM\n",
      "Flag                             \n",
      "subject_has_currency   0.19  3.80\n",
      "subject_has_fwd        4.52  2.67\n",
      "subject_has_re        19.20  5.34\n",
      "   Class                          Indicator    Median [Q1-Q3]\n",
      "0   SPAM                     subject_has_re  0.00 [0.00-0.00]\n",
      "1   SPAM                    subject_has_fwd  0.00 [0.00-0.00]\n",
      "2   SPAM          subject_exclamation_count  0.00 [0.00-0.00]\n",
      "3   SPAM                subject_digit_count  0.00 [0.00-0.00]\n",
      "4   SPAM               subject_has_currency  0.00 [0.00-0.00]\n",
      "5   SPAM  subject_percent_uppercase_letters  0.00 [0.00-0.00]\n",
      "6    HAM                     subject_has_re  0.00 [0.00-0.00]\n",
      "7    HAM                    subject_has_fwd  0.00 [0.00-0.00]\n",
      "8    HAM          subject_exclamation_count  0.00 [0.00-0.00]\n",
      "9    HAM                subject_digit_count  1.00 [0.00-5.00]\n",
      "10   HAM               subject_has_currency  0.00 [0.00-0.00]\n",
      "11   HAM  subject_percent_uppercase_letters  0.00 [0.00-0.00]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” Subject indicators by class\n",
    "\n",
    "# Calculate subject indicators\n",
    "# --- PATCH: subject indicators ---\n",
    "\n",
    "# Pattern piÃ¹ robusti per \"Re:\" e \"Fwd:\" con spazi opzionali\n",
    "RE_PAT  = r'(?i)\\bre\\s*:'\n",
    "FWD_PAT = r'(?i)\\b(?:fw|fwd)\\s*:'\n",
    "\n",
    "def calculate_subject_indicators(subject: str):\n",
    "    subject = (subject or \"\").strip()\n",
    "    # Conta solo lettere per la % maiuscole\n",
    "    letters = [c for c in subject if c.isalpha()]\n",
    "    n_letters = max(1, len(letters))\n",
    "    uppercase_letters = sum(1 for c in letters if c.isupper())\n",
    "\n",
    "    return {\n",
    "        \"subject_has_re\": 1 if re.search(RE_PAT, subject) else 0,\n",
    "        \"subject_has_fwd\": 1 if re.search(FWD_PAT, subject) else 0,\n",
    "        \"subject_exclamation_count\": subject.count(\"!\"),\n",
    "        \"subject_digit_count\": sum(c.isdigit() for c in subject),\n",
    "        \"subject_has_currency\": 1 if re.search(r\"[$â‚¬Â£Â¥]\", subject) else 0,\n",
    "        \"subject_percent_uppercase_letters\": (uppercase_letters / n_letters) * 100.0,\n",
    "    }\n",
    "\n",
    "print(\"Calculating subject indicators...\")\n",
    "subject_indicators = df[\"subject\"].apply(calculate_subject_indicators)\n",
    "\n",
    "cols = [\n",
    "    \"subject_has_re\", \"subject_has_fwd\", \"subject_exclamation_count\",\n",
    "    \"subject_digit_count\", \"subject_has_currency\",\n",
    "    \"subject_percent_uppercase_letters\"\n",
    "]\n",
    "for c in cols:\n",
    "    df[c] = subject_indicators.apply(lambda x: x[c])\n",
    "\n",
    "# Tabella mediana [Q1â€“Q3] (come giÃ  fai)\n",
    "results = []\n",
    "for label_num, class_label in [(1,\"SPAM\"), (0,\"HAM\")]:\n",
    "    class_df = df[df[\"label_num\"] == label_num]\n",
    "    for c in cols:\n",
    "        med = class_df[c].median()\n",
    "        q1 = class_df[c].quantile(0.25)\n",
    "        q3 = class_df[c].quantile(0.75)\n",
    "        results.append({\n",
    "            \"Class\": class_label,\n",
    "            \"Indicator\": c,\n",
    "            \"Median [Q1-Q3]\": f\"{med:.2f} [{q1:.2f}-{q3:.2f}]\"\n",
    "        })\n",
    "indicators_df = pd.DataFrame(results)\n",
    "print(\"\\n=== SUBJECT INDICATORS BY CLASS (mediana [IQR]) ===\")\n",
    "print(indicators_df[[\"Class\",\"Indicator\",\"Median [Q1-Q3]\"]])\n",
    "\n",
    "# ðŸš© Aggiungi ANCHE la prevalenza (%) per i flag binari: piÃ¹ informativa della mediana\n",
    "flag_cols = [\"subject_has_re\",\"subject_has_fwd\",\"subject_has_currency\"]\n",
    "pct_rows = []\n",
    "for label_num, class_label in [(1,\"SPAM\"), (0,\"HAM\")]:\n",
    "    class_df = df[df[\"label_num\"] == label_num]\n",
    "    for c in flag_cols:\n",
    "        pct = class_df[c].mean() * 100.0\n",
    "        pct_rows.append({\"Class\": class_label, \"Flag\": c, \"Prevalence %\": f\"{pct:.2f}\"})\n",
    "pct_df = pd.DataFrame(pct_rows)\n",
    "print(\"\\n=== SUBJECT FLAGS PREVALENCE (percentuale di True) ===\")\n",
    "print(pct_df.pivot(index=\"Flag\", columns=\"Class\", values=\"Prevalence %\"))\n",
    "\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "indicators_df = pd.DataFrame(results)\n",
    "print(indicators_df[['Class', 'Indicator', 'Median [Q1-Q3]']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11fd694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating content indicators for body and full text...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BODY CONTENT INDICATORS: SPAM vs HAM ===\n",
      "                      Metric SPAM (median [IQR])  HAM (median [IQR])  \\\n",
      "0                  url_count    0.00 [0.00-0.00]    0.00 [0.00-0.00]   \n",
      "1                email_count    0.00 [0.00-0.00]    0.00 [0.00-0.00]   \n",
      "2                phone_count    0.00 [0.00-0.00]    0.00 [0.00-2.00]   \n",
      "3                digit_count   5.00 [0.00-20.00]  24.00 [6.00-53.00]   \n",
      "4          exclamation_count    0.00 [0.00-2.00]    0.00 [0.00-0.00]   \n",
      "5  percent_uppercase_letters    0.00 [0.00-0.00]    0.00 [0.00-0.00]   \n",
      "6            spam_word_count    0.00 [0.00-1.00]    0.00 [0.00-0.00]   \n",
      "\n",
      "  SPAM/HAM ratio  \n",
      "0          0.00x  \n",
      "1          0.00x  \n",
      "2          0.00x  \n",
      "3          0.21x  \n",
      "4          0.00x  \n",
      "5          0.00x  \n",
      "6          0.00x  \n"
     ]
    }
   ],
   "source": [
    "# Cell 4 â€” Simple signals on body and text\n",
    "\n",
    "# Define function to calculate content indicators\n",
    "def calculate_content_indicators(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    \n",
    "    # Count letters for uppercase percentage calculation\n",
    "    letters = [c for c in text if c.isalpha()]\n",
    "    letter_count = max(1, len(letters))\n",
    "    uppercase_letters = sum(1 for c in letters if c.isupper())\n",
    "    \n",
    "    # Define spam words list\n",
    "    spam_words = ['free', 'offer', 'money', 'click', 'win', 'cash', 'prize', 'discount']\n",
    "    \n",
    "    # Calculate indicators\n",
    "    url_count = len(re.findall(r'https?://\\S+|www\\.\\S+', text))\n",
    "    email_count = len(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text))\n",
    "    phone_count = len(re.findall(r'\\+?\\d[\\d\\s\\-\\(\\)]{7,}', text))\n",
    "    digit_count = sum(c.isdigit() for c in text)\n",
    "    exclamation_count = text.count('!')\n",
    "    percent_uppercase = (uppercase_letters / letter_count) * 100\n",
    "    \n",
    "    # Count spam words (case insensitive)\n",
    "    spam_word_count = sum(len(re.findall(r'\\b' + word + r'\\b', text, re.IGNORECASE)) for word in spam_words)\n",
    "    \n",
    "    return {\n",
    "        'url_count': url_count,\n",
    "        'email_count': email_count,\n",
    "        'phone_count': phone_count,\n",
    "        'digit_count': digit_count,\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'percent_uppercase_letters': percent_uppercase,\n",
    "        'spam_word_count': spam_word_count\n",
    "    }\n",
    "\n",
    "# Apply to body and text\n",
    "print(\"Calculating content indicators for body and full text...\")\n",
    "\n",
    "# Process body\n",
    "body_indicators = df['body'].apply(calculate_content_indicators)\n",
    "for indicator, values in zip(\n",
    "    ['body_url_count', 'body_email_count', 'body_phone_count', 'body_digit_count',\n",
    "     'body_exclamation_count', 'body_percent_uppercase_letters', 'body_spam_word_count'],\n",
    "    ['url_count', 'email_count', 'phone_count', 'digit_count',\n",
    "     'exclamation_count', 'percent_uppercase_letters', 'spam_word_count']\n",
    "):\n",
    "    df[indicator] = body_indicators.apply(lambda x: x[values])\n",
    "\n",
    "# Process full text\n",
    "text_indicators = df['text'].apply(calculate_content_indicators)\n",
    "for indicator, values in zip(\n",
    "    ['text_url_count', 'text_email_count', 'text_phone_count', 'text_digit_count',\n",
    "     'text_exclamation_count', 'text_percent_uppercase_letters', 'text_spam_word_count'],\n",
    "    ['url_count', 'email_count', 'phone_count', 'digit_count',\n",
    "     'exclamation_count', 'percent_uppercase_letters', 'spam_word_count']\n",
    "):\n",
    "    df[indicator] = text_indicators.apply(lambda x: x[values])\n",
    "\n",
    "# Print comparative table for body metrics\n",
    "print(\"\\n=== BODY CONTENT INDICATORS: SPAM vs HAM ===\")\n",
    "\n",
    "body_metrics = ['body_url_count', 'body_email_count', 'body_phone_count', \n",
    "               'body_digit_count', 'body_exclamation_count', \n",
    "               'body_percent_uppercase_letters', 'body_spam_word_count']\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for metric in body_metrics:\n",
    "    # SPAM statistics\n",
    "    spam_median = df[df['label_num'] == 1][metric].median()\n",
    "    spam_q1 = df[df['label_num'] == 1][metric].quantile(0.25)\n",
    "    spam_q3 = df[df['label_num'] == 1][metric].quantile(0.75)\n",
    "    spam_iqr = f\"{spam_median:.2f} [{spam_q1:.2f}-{spam_q3:.2f}]\"\n",
    "    \n",
    "    # HAM statistics\n",
    "    ham_median = df[df['label_num'] == 0][metric].median()\n",
    "    ham_q1 = df[df['label_num'] == 0][metric].quantile(0.25)\n",
    "    ham_q3 = df[df['label_num'] == 0][metric].quantile(0.75)\n",
    "    ham_iqr = f\"{ham_median:.2f} [{ham_q1:.2f}-{ham_q3:.2f}]\"\n",
    "    \n",
    "    # Calculate ratio (protect against division by zero)\n",
    "    ratio = spam_median / max(0.001, ham_median)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Metric': metric.replace('body_', ''),\n",
    "        'SPAM (median [IQR])': spam_iqr,\n",
    "        'HAM (median [IQR])': ham_iqr,\n",
    "        'SPAM/HAM ratio': f\"{ratio:.2f}x\"\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5082684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting formatting and noise patterns...\n",
      "\n",
      "=== FORMATTING & NOISE PATTERNS BY CLASS ===\n",
      "                   Flag SPAM %  HAM %\n",
      "0              has_html  0.00%  0.00%\n",
      "1  has_obfuscated_links  0.00%  0.00%\n",
      "2         has_signature  0.00%  0.00%\n",
      "3        has_disclaimer  2.47%  0.95%\n",
      "4  has_original_message  0.00%  0.00%\n",
      "\n",
      "DECISION: Will propose to extract meta-features from HTML/formatting before cleaning, then remove HTML tags and normalize text.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 â€” Formatting & \"noise\" flags\n",
    "\n",
    "# Define functions to detect formatting and noise patterns\n",
    "def has_html(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    return bool(re.search(r'<[^>]+>', text))\n",
    "\n",
    "def has_obfuscated_links(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    \n",
    "    # Find all anchor tags\n",
    "    anchor_tags = re.findall(r'<a\\s+[^>]*href\\s*=\\s*[\"\\']([^\"\\']+)[\"\\'][^>]*>(.*?)</a>', text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    for href, anchor_text in anchor_tags:\n",
    "        # Extract domains\n",
    "        href_domain = re.search(r'https?://([^/]+)', href)\n",
    "        anchor_domain = re.search(r'https?://([^/]+)', anchor_text)\n",
    "        \n",
    "        # Check if anchor text doesn't contain a domain (generic text like \"click here\")\n",
    "        if not anchor_domain:\n",
    "            return True\n",
    "            \n",
    "        # Check if domains don't match\n",
    "        if href_domain and anchor_domain and href_domain.group(1) != anchor_domain.group(1):\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "def has_signature(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    return bool(re.search(r'--\\s*\\n', text))\n",
    "\n",
    "def has_disclaimer(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    return bool(re.search(r'(?i)\\b(disclaimer|legal notice|confidential)\\b', text))\n",
    "\n",
    "def has_original_message(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    pattern1 = r'(?im)^-{2,}\\s*Original Message\\s*-{2,}'\n",
    "    pattern2 = r'(?ims)^\\s*From: .+\\n\\s*Sent: .+\\n\\s*To: .+\\n\\s*Subject: .+'\n",
    "    return bool(re.search(pattern1, text) or re.search(pattern2, text))\n",
    "\n",
    "# Apply functions to add flags\n",
    "print(\"Detecting formatting and noise patterns...\")\n",
    "df['has_html'] = df['text'].apply(has_html)\n",
    "df['has_obfuscated_links'] = df['text'].apply(has_obfuscated_links)\n",
    "df['has_signature'] = df['text'].apply(has_signature)\n",
    "df['has_disclaimer'] = df['text'].apply(has_disclaimer)\n",
    "df['has_original_message'] = df['text'].apply(has_original_message)\n",
    "\n",
    "# Print percentages by class\n",
    "print(\"\\n=== FORMATTING & NOISE PATTERNS BY CLASS ===\")\n",
    "\n",
    "flags = ['has_html', 'has_obfuscated_links', 'has_signature', 'has_disclaimer', 'has_original_message']\n",
    "results = []\n",
    "\n",
    "for flag in flags:\n",
    "    # Calculate percentages\n",
    "    spam_pct = df[df['label_num'] == 1][flag].mean() * 100\n",
    "    ham_pct = df[df['label_num'] == 0][flag].mean() * 100\n",
    "    \n",
    "    results.append({\n",
    "        'Flag': flag,\n",
    "        'SPAM %': f\"{spam_pct:.2f}%\",\n",
    "        'HAM %': f\"{ham_pct:.2f}%\"\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "flags_df = pd.DataFrame(results)\n",
    "print(flags_df)\n",
    "\n",
    "print(\"\\nDECISION: Will propose to extract meta-features from HTML/formatting before cleaning, then remove HTML tags and normalize text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decf1aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for anti-spam headers (potential leakage)...\n",
      "\n",
      "=== LEAKAGE DETECTION (ANTI-SPAM HEADERS) ===\n",
      "Emails with potential leakage: 0 (0.00% of dataset)\n",
      "\n",
      "No anti-spam headers detected. No special handling required.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” Leakage (anti-spam headers)\n",
    "\n",
    "# Define function to check for leakage\n",
    "def check_leakage(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    leakage_patterns = [\n",
    "        r'X-Spam-Flag:',\n",
    "        r'Spam-Score:',\n",
    "        r'SpamAssassin',\n",
    "        r'X-Spam-Status:',\n",
    "        r'X-Spam-Level:'\n",
    "    ]\n",
    "    \n",
    "    for pattern in leakage_patterns:\n",
    "        if re.search(f'(?i){pattern}', text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Apply function to add flag\n",
    "print(\"Checking for anti-spam headers (potential leakage)...\")\n",
    "df['has_leakage'] = df['text'].apply(check_leakage)\n",
    "\n",
    "# Calculate statistics\n",
    "leakage_count = df['has_leakage'].sum()\n",
    "leakage_percent = leakage_count / len(df) * 100\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== LEAKAGE DETECTION (ANTI-SPAM HEADERS) ===\")\n",
    "print(f\"Emails with potential leakage: {leakage_count} ({leakage_percent:.2f}% of dataset)\")\n",
    "\n",
    "if leakage_count > 0:\n",
    "    print(\"\\nWARNING: Found anti-spam headers that could cause data leakage!\")\n",
    "    print(\"DECISION: These headers should be excluded during preprocessing in Step 3.\")\n",
    "else:\n",
    "    print(\"\\nNo anti-spam headers detected. No special handling required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d1345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating top 20 n-grams for each corpus...\n",
      "\n",
      "=== TOP UNIGRAMS IN SUBJECT ===\n",
      "SPAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>software</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>online</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meds</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paliourg</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cheap</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>want</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>free</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>best</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prices</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xp</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>low</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>price</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>buy</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>download</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fwd</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>viagra</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>need</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cialis</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>th</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ngram  frequency\n",
       "0   software         44\n",
       "1     online         42\n",
       "2       meds         41\n",
       "3        new         41\n",
       "4   paliourg         36\n",
       "5      cheap         33\n",
       "6       want         31\n",
       "7       free         31\n",
       "8       best         30\n",
       "9     prices         27\n",
       "10        xp         27\n",
       "11       low         25\n",
       "12     price         25\n",
       "13       buy         25\n",
       "14  download         24\n",
       "15       fwd         23\n",
       "16    viagra         22\n",
       "17      need         22\n",
       "18    cialis         22\n",
       "19        th         21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hpl</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nom</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meter</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enron</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gas</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>actuals</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nomination</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fw</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noms</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deal</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tenaska</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>april</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>change</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>march</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>june</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>july</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eastrans</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>revised</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>new</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>iv</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ngram  frequency\n",
       "0          hpl        582\n",
       "1          nom        338\n",
       "2        meter        333\n",
       "3        enron        276\n",
       "4          gas        232\n",
       "5      actuals        214\n",
       "6   nomination        209\n",
       "7           fw        177\n",
       "8         noms        148\n",
       "9         deal        134\n",
       "10     tenaska        107\n",
       "11       april        101\n",
       "12      change        100\n",
       "13       march        100\n",
       "14        june         95\n",
       "15        july         93\n",
       "16    eastrans         91\n",
       "17     revised         83\n",
       "18         new         83\n",
       "19          iv         80"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP BIGRAMS IN SUBJECT ===\n",
      "SPAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>instant download</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soft tabs</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>charset ascii</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>windows xp</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>office xp</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi paliourg</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prescription needed</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>penis growth</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fix penis</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cialis soft</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>low prices</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>download date</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>download coupon</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>write review</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>solution penis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rom download</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ise media</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>average customer</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>june th</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>price save</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ngram  frequency\n",
       "0      instant download         13\n",
       "1             soft tabs         12\n",
       "2         charset ascii         11\n",
       "3            windows xp         10\n",
       "4             office xp         10\n",
       "5           hi paliourg          9\n",
       "6   prescription needed          7\n",
       "7          penis growth          7\n",
       "8             fix penis          7\n",
       "9           cialis soft          7\n",
       "10           low prices          7\n",
       "11        download date          6\n",
       "12      download coupon          6\n",
       "13         write review          6\n",
       "14       solution penis          6\n",
       "15         rom download          6\n",
       "16            ise media          6\n",
       "17     average customer          6\n",
       "18              june th          6\n",
       "19           price save          6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hpl nom</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enron hpl</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hpl actuals</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tenaska iv</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gas nomination</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>calpine daily</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eastrans nomination</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daily gas</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noms actual</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actual flow</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hpl meter</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hpl noms</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nomination change</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>enron actuals</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>change effective</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nom march</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>actuals august</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>actuals july</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>natural gas</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nom april</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ngram  frequency\n",
       "0               hpl nom        234\n",
       "1             enron hpl        185\n",
       "2           hpl actuals        168\n",
       "3            tenaska iv         80\n",
       "4        gas nomination         70\n",
       "5         calpine daily         54\n",
       "6   eastrans nomination         53\n",
       "7             daily gas         49\n",
       "8           noms actual         45\n",
       "9           actual flow         44\n",
       "10            hpl meter         43\n",
       "11             hpl noms         40\n",
       "12    nomination change         39\n",
       "13        enron actuals         36\n",
       "14     change effective         35\n",
       "15            nom march         34\n",
       "16       actuals august         28\n",
       "17         actuals july         28\n",
       "18          natural gas         26\n",
       "19            nom april         24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP UNIGRAMS IN BODY ===\n",
      "SPAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http</td>\n",
       "      <td>983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>company</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>information</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>font</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>td</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>statements</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>email</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>price</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nbsp</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>new</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>height</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>time</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>width</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pills</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>size</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>message</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>investment</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>free</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ngram  frequency\n",
       "0           com        989\n",
       "1          http        983\n",
       "2       company        723\n",
       "3           www        587\n",
       "4   information        513\n",
       "5          font        511\n",
       "6            td        504\n",
       "7    statements        476\n",
       "8         email        471\n",
       "9         price        446\n",
       "10         nbsp        418\n",
       "11          new        391\n",
       "12       height        362\n",
       "13         time        342\n",
       "14        width        306\n",
       "15        pills        303\n",
       "16         size        301\n",
       "17      message        285\n",
       "18   investment        284\n",
       "19         free        282"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ect</td>\n",
       "      <td>13893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hou</td>\n",
       "      <td>7281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enron</td>\n",
       "      <td>6279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject</td>\n",
       "      <td>2728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com</td>\n",
       "      <td>2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deal</td>\n",
       "      <td>2655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gas</td>\n",
       "      <td>2629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cc</td>\n",
       "      <td>2357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pm</td>\n",
       "      <td>2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meter</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>daren</td>\n",
       "      <td>1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hpl</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>corp</td>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>know</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mmbtu</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>forwarded</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>need</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>farmer</td>\n",
       "      <td>1137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>let</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ngram  frequency\n",
       "0         ect      13893\n",
       "1         hou       7281\n",
       "2       enron       6279\n",
       "3     subject       2728\n",
       "4         com       2693\n",
       "5        deal       2655\n",
       "6         gas       2629\n",
       "7          cc       2357\n",
       "8          pm       2325\n",
       "9       meter       2126\n",
       "10      daren       1897\n",
       "11     thanks       1810\n",
       "12        hpl       1736\n",
       "13       corp       1701\n",
       "14       know       1436\n",
       "15      mmbtu       1345\n",
       "16  forwarded       1292\n",
       "17       need       1254\n",
       "18     farmer       1137\n",
       "19        let       1083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP BIGRAMS IN BODY ===\n",
      "SPAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http www</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nbsp nbsp</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>computron com</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>href http</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>looking statements</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pills pills</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>width height</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>src http</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>www computron</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>forward looking</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http nd</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>td td</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>investment advice</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>font size</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>td tr</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tr td</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>align center</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>size pt</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>duty free</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ali duty</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ngram  frequency\n",
       "0             http www        409\n",
       "1            nbsp nbsp        296\n",
       "2        computron com        195\n",
       "3            href http        175\n",
       "4   looking statements        172\n",
       "5          pills pills        169\n",
       "6         width height        164\n",
       "7             src http        157\n",
       "8        www computron        152\n",
       "9      forward looking        142\n",
       "10             http nd        136\n",
       "11               td td        134\n",
       "12   investment advice        118\n",
       "13           font size        113\n",
       "14               td tr        111\n",
       "15               tr td         94\n",
       "16        align center         91\n",
       "17             size pt         87\n",
       "18           duty free         82\n",
       "19            ali duty         81"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HAM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hou ect</td>\n",
       "      <td>7226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ect ect</td>\n",
       "      <td>6339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enron enron</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ect cc</td>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corp enron</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cc subject</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>let know</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daren farmer</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>enron com</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ect subject</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ect pm</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>farmer hou</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>attached file</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>original message</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pec pec</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>teco tap</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>enron cc</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ami chokshi</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gas daily</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>north america</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ngram  frequency\n",
       "0            hou ect       7226\n",
       "1            ect ect       6339\n",
       "2        enron enron       1440\n",
       "3             ect cc       1391\n",
       "4         corp enron       1210\n",
       "5         cc subject       1099\n",
       "6           let know        980\n",
       "7       daren farmer        933\n",
       "8          enron com        824\n",
       "9        ect subject        730\n",
       "10            ect pm        567\n",
       "11        farmer hou        563\n",
       "12     attached file        501\n",
       "13  original message        437\n",
       "14           pec pec        357\n",
       "15          teco tap        332\n",
       "16          enron cc        314\n",
       "17       ami chokshi        304\n",
       "18         gas daily        303\n",
       "19     north america        301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7 â€” Distinctive lexicon (top n-grams)\n",
    "\n",
    "# Define preprocessing function for n-grams\n",
    "def preprocess_for_ngrams(text):\n",
    "    text = str(text).strip() if pd.notna(text) else ''\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to get top n-grams\n",
    "def get_top_ngrams(corpus, n, top_k=20):\n",
    "    corpus = [preprocess_for_ngrams(text) for text in corpus]\n",
    "    \n",
    "    # Skip empty texts\n",
    "    corpus = [text for text in corpus if text.strip()]\n",
    "    \n",
    "    if not corpus:\n",
    "        return pd.DataFrame({'ngram': [], 'frequency': []})\n",
    "    \n",
    "    # Create vectorizer and fit\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Get feature names and frequencies\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    frequencies = X.sum(axis=0).A1\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    ngram_df = pd.DataFrame({'ngram': feature_names, 'frequency': frequencies})\n",
    "    ngram_df = ngram_df.sort_values('frequency', ascending=False).head(top_k).reset_index(drop=True)\n",
    "    \n",
    "    return ngram_df\n",
    "\n",
    "# Set top K parameter\n",
    "TOP_K = 20\n",
    "print(f\"Generating top {TOP_K} n-grams for each corpus...\")\n",
    "\n",
    "# Split data by class\n",
    "spam_subjects = df[df['label_num'] == 1]['subject']\n",
    "ham_subjects = df[df['label_num'] == 0]['subject']\n",
    "spam_bodies = df[df['label_num'] == 1]['body']\n",
    "ham_bodies = df[df['label_num'] == 0]['body']\n",
    "\n",
    "# Generate top n-grams\n",
    "print(\"\\n=== TOP UNIGRAMS IN SUBJECT ===\")\n",
    "print(\"SPAM:\")\n",
    "display(get_top_ngrams(spam_subjects, 1, TOP_K))\n",
    "print(\"\\nHAM:\")\n",
    "display(get_top_ngrams(ham_subjects, 1, TOP_K))\n",
    "\n",
    "print(\"\\n=== TOP BIGRAMS IN SUBJECT ===\")\n",
    "print(\"SPAM:\")\n",
    "display(get_top_ngrams(spam_subjects, 2, TOP_K))\n",
    "print(\"\\nHAM:\")\n",
    "display(get_top_ngrams(ham_subjects, 2, TOP_K))\n",
    "\n",
    "print(\"\\n=== TOP UNIGRAMS IN BODY ===\")\n",
    "print(\"SPAM:\")\n",
    "display(get_top_ngrams(spam_bodies, 1, TOP_K))\n",
    "print(\"\\nHAM:\")\n",
    "display(get_top_ngrams(ham_bodies, 1, TOP_K))\n",
    "\n",
    "print(\"\\n=== TOP BIGRAMS IN BODY ===\")\n",
    "print(\"SPAM:\")\n",
    "display(get_top_ngrams(spam_bodies, 2, TOP_K))\n",
    "print(\"\\nHAM:\")\n",
    "display(get_top_ngrams(ham_bodies, 2, TOP_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf3a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 2: DEFINITION OF DONE ===\n",
      "\n",
      "1. DUPLICATES:\n",
      "â€¢ Rows marked as duplicates: 321 (6.21% of dataset)\n",
      "â€¢ Number of duplicate groups: 143\n",
      "â€¢ Breakdown by class:\n",
      "  - ham: 264 (7.19% of class)\n",
      "  - spam: 57 (3.80% of class)\n",
      "\n",
      "2. SUBJECT/BODY STATISTICS (median [IQR]):\n",
      "â€¢ subject_length:\n",
      "  - SPAM: 32.00 [20.00-46.00]\n",
      "  - HAM: 28.00 [19.00-40.00]\n",
      "â€¢ body_length:\n",
      "  - SPAM: 507.00 [227.50-1171.00]\n",
      "  - HAM: 473.00 [190.00-1170.25]\n",
      "â€¢ subject_token_count:\n",
      "  - SPAM: 7.00 [4.00-10.00]\n",
      "  - HAM: 6.00 [4.00-9.00]\n",
      "â€¢ body_token_count:\n",
      "  - SPAM: 105.00 [46.00-232.00]\n",
      "  - HAM: 118.00 [44.00-284.00]\n",
      "â€¢ subject_body_ratio:\n",
      "  - SPAM: 0.06 [0.02-0.15]\n",
      "  - HAM: 0.05 [0.02-0.15]\n",
      "\n",
      "2.1. SUBJECT FLAGS PREVALENCE (%):\n",
      "Class                   HAM  SPAM\n",
      "Flag                             \n",
      "subject_has_currency   0.19  3.80\n",
      "subject_has_fwd        4.52  2.67\n",
      "subject_has_re        19.20  5.34\n",
      "\n",
      "3. BODY SIGNALS COMPARISON (median [IQR]):\n",
      "                   Metric SPAM (median [IQR]) HAM (median [IQR]) SPAM/HAM ratio\n",
      "                url_count    0.00 [0.00-0.00]   0.00 [0.00-0.00]          0.00x\n",
      "              email_count    0.00 [0.00-0.00]   0.00 [0.00-0.00]          0.00x\n",
      "              phone_count    0.00 [0.00-0.00]   0.00 [0.00-2.00]          0.00x\n",
      "              digit_count   5.00 [0.00-20.00] 24.00 [6.00-53.00]          0.21x\n",
      "        exclamation_count    0.00 [0.00-2.00]   0.00 [0.00-0.00]          0.00x\n",
      "percent_uppercase_letters    0.00 [0.00-0.00]   0.00 [0.00-0.00]          0.00x\n",
      "          spam_word_count    0.00 [0.00-1.00]   0.00 [0.00-0.00]          0.00x\n",
      "\n",
      "3.1. BODY SIGNALS: PREVALENCE, CONDITIONAL MEAN, AND P95:\n",
      "  Class % url_count>0 % email_count>0 % phone_count>0 % digit_count>0  \\\n",
      "0  SPAM         0.00%           0.00%          21.21%          70.65%   \n",
      "1   HAM         0.00%           0.00%          49.73%          91.09%   \n",
      "\n",
      "  % exclamation_count>0 % spam_word_count>0 mean url_count|>0  \\\n",
      "0                45.83%              37.16%               nan   \n",
      "1                12.96%               8.80%               nan   \n",
      "\n",
      "  mean email_count|>0 mean phone_count|>0 mean digit_count|>0  \\\n",
      "0                 nan                2.84               36.30   \n",
      "1                 nan                4.01               47.06   \n",
      "\n",
      "  mean exclamation_count|>0 mean spam_word_count|>0 p95 url_count  \\\n",
      "0                      3.21                    2.09             0   \n",
      "1                      2.29                    1.95             0   \n",
      "\n",
      "  p95 email_count p95 phone_count p95 digit_count p95 exclamation_count  \\\n",
      "0               0               3             109                     6   \n",
      "1               0               8             147                     2   \n",
      "\n",
      "  p95 spam_word_count  \n",
      "0                   3  \n",
      "1                   1  \n",
      "\n",
      "4. FORMATTING & NOISE:\n",
      "â€¢ has_html:\n",
      "  - SPAM: 0.00%\n",
      "  - HAM: 0.00%\n",
      "â€¢ has_obfuscated_links:\n",
      "  - SPAM: 0.00%\n",
      "  - HAM: 0.00%\n",
      "â€¢ has_signature:\n",
      "  - SPAM: 0.00%\n",
      "  - HAM: 0.00%\n",
      "â€¢ has_disclaimer:\n",
      "  - SPAM: 2.47%\n",
      "  - HAM: 0.95%\n",
      "â€¢ has_original_message:\n",
      "  - SPAM: 0.00%\n",
      "  - HAM: 0.00%\n",
      "\n",
      "5. LEAKAGE:\n",
      "â€¢ Anti-spam headers: 0 emails (0.00% of dataset)\n",
      "â€¢ No leakage detected\n",
      "\n",
      "6. DECISIONS FOR STEP 3:\n",
      "â€¢ Remove exact duplicates in training, keeping one instance per group\n",
      "â€¢ Extract meta-features from HTML/URL/email/phone before cleanup\n",
      "â€¢ Remove HTML tags and normalize whitespace/case\n",
      "â€¢ Apply PII placeholders: [URL] [EMAIL] [PHONE] [NUMBER]\n",
      "â€¢ Use weighted combination of subject + body\n",
      "â€¢ Truncate content after signatures and 'Original Message' blocks\n",
      "â€¢ Usa class_weight=\"balanced\" nel modello (gestisce automaticamente lo sbilanciamento).\n",
      "â€¢ Escludi eventuali header antispam (X-Spam-Flag, Spam-Score, X-Spam-Status, X-Spam-Level, SpamAssassin) dal testo per il modello (qui non rilevati, ma la regola resta).\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 â€” \"Definition of Done\" for Step 2 (con analisi aggiuntive)\n",
    "\n",
    "print(\"=== STEP 2: DEFINITION OF DONE ===\")\n",
    "\n",
    "# 1. Duplicates summary\n",
    "print(\"\\n1. DUPLICATES:\")\n",
    "print(f\"â€¢ Rows marked as duplicates: {n_rows_dup_marked} ({n_rows_dup_marked/len(df)*100:.2f}% of dataset)\")\n",
    "print(f\"â€¢ Number of duplicate groups: {n_groups_dup}\")\n",
    "print(\"â€¢ Breakdown by class:\")\n",
    "for label, count in dup_by_class.items():\n",
    "    print(f\"  - {label}: {count} ({dup_pct_by_class[label]:.2f}% of class)\")\n",
    "\n",
    "# 2. Subject/Body statistics\n",
    "print(\"\\n2. SUBJECT/BODY STATISTICS (median [IQR]):\")\n",
    "for metric in metrics:\n",
    "    spam_data = df[df['label_num'] == 1][metric]\n",
    "    ham_data = df[df['label_num'] == 0][metric]\n",
    "    \n",
    "    spam_median = spam_data.median()\n",
    "    spam_q1 = spam_data.quantile(0.25)\n",
    "    spam_q3 = spam_data.quantile(0.75)\n",
    "    \n",
    "    ham_median = ham_data.median()\n",
    "    ham_q1 = ham_data.quantile(0.25)\n",
    "    ham_q3 = ham_data.quantile(0.75)\n",
    "    \n",
    "    print(f\"â€¢ {metric}:\")\n",
    "    print(f\"  - SPAM: {spam_median:.2f} [{spam_q1:.2f}-{spam_q3:.2f}]\")\n",
    "    print(f\"  - HAM: {ham_median:.2f} [{ham_q1:.2f}-{ham_q3:.2f}]\")\n",
    "\n",
    "# 2.1 Subject flags prevalence\n",
    "print(\"\\n2.1. SUBJECT FLAGS PREVALENCE (%):\")\n",
    "flag_cols = [\"subject_has_re\",\"subject_has_fwd\",\"subject_has_currency\"]\n",
    "rows = []\n",
    "for lbl, name in [(1,\"SPAM\"), (0,\"HAM\")]:\n",
    "    sub = df.loc[df.label_num==lbl, flag_cols]\n",
    "    for c in flag_cols:\n",
    "        rows.append({\"Class\": name, \"Flag\": c, \"Prevalence %\": round(sub[c].mean()*100, 2)})\n",
    "prevalence_df = pd.DataFrame(rows).pivot(index=\"Flag\", columns=\"Class\", values=\"Prevalence %\")\n",
    "print(prevalence_df)\n",
    "\n",
    "# 3. Body signals comparison\n",
    "print(\"\\n3. BODY SIGNALS COMPARISON (median [IQR]):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 3.1 Body signals: detailed stats\n",
    "print(\"\\n3.1. BODY SIGNALS: PREVALENCE, CONDITIONAL MEAN, AND P95:\")\n",
    "count_cols = [\"body_url_count\",\"body_email_count\",\"body_phone_count\",\n",
    "              \"body_digit_count\",\"body_exclamation_count\",\"body_spam_word_count\"]\n",
    "out_rows = []\n",
    "for lbl, name in [(1,\"SPAM\"), (0,\"HAM\")]:\n",
    "    sub = df.loc[df.label_num==lbl, count_cols]\n",
    "    gt0 = (sub > 0).mean()*100\n",
    "    cond_mean = sub.where(sub>0).mean()\n",
    "    p95 = sub.quantile(0.95)\n",
    "    out = {\"Class\": name}\n",
    "    out.update({f\"% {c.split('_',1)[1]}>0\": f\"{gt0[c]:.2f}%\" for c in count_cols})\n",
    "    out.update({f\"mean {c.split('_',1)[1]}|>0\": f\"{cond_mean[c]:.2f}\" for c in count_cols})\n",
    "    out.update({f\"p95 {c.split('_',1)[1]}\": f\"{p95[c]:.0f}\" for c in count_cols})\n",
    "    out_rows.append(out)\n",
    "detailed_stats_df = pd.DataFrame(out_rows)\n",
    "print(detailed_stats_df)\n",
    "\n",
    "# 4. Formatting percentages\n",
    "print(\"\\n4. FORMATTING & NOISE:\")\n",
    "for flag in flags:\n",
    "    spam_pct = df[df['label_num'] == 1][flag].mean() * 100\n",
    "    ham_pct = df[df['label_num'] == 0][flag].mean() * 100\n",
    "    \n",
    "    print(f\"â€¢ {flag}:\")\n",
    "    print(f\"  - SPAM: {spam_pct:.2f}%\")\n",
    "    print(f\"  - HAM: {ham_pct:.2f}%\")\n",
    "\n",
    "# 5. Leakage\n",
    "print(\"\\n5. LEAKAGE:\")\n",
    "print(f\"â€¢ Anti-spam headers: {leakage_count} emails ({leakage_percent:.2f}% of dataset)\")\n",
    "if leakage_count > 0:\n",
    "    print(\"â€¢ DECISION: Exclude anti-spam headers during preprocessing in Step 3\")\n",
    "else:\n",
    "    print(\"â€¢ No leakage detected\")\n",
    "\n",
    "# 6. Decisions for Step 3\n",
    "print(\"\\n6. DECISIONS FOR STEP 3:\")\n",
    "print(\"â€¢ Remove exact duplicates in training, keeping one instance per group\")\n",
    "print(\"â€¢ Extract meta-features from HTML/URL/email/phone before cleanup\")\n",
    "print(\"â€¢ Remove HTML tags and normalize whitespace/case\")\n",
    "print(\"â€¢ Apply PII placeholders: [URL] [EMAIL] [PHONE] [NUMBER]\")\n",
    "print(\"â€¢ Use weighted combination of subject + body\")\n",
    "print(\"â€¢ Truncate content after signatures and 'Original Message' blocks\")\n",
    "print(\"â€¢ Usa class_weight=\\\"balanced\\\" nel modello (gestisce automaticamente lo sbilanciamento).\")\n",
    "print(\"â€¢ Escludi eventuali header antispam (X-Spam-Flag, Spam-Score, X-Spam-Status, X-Spam-Level, SpamAssassin) dal testo per il modello (qui non rilevati, ma la regola resta).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2c125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESSING SETUP ===\n",
      "â€¢ Random Seed: 42\n",
      "â€¢ Cross-validation folds: 5\n",
      "â€¢ Subject weight: 2x\n",
      "\n",
      "=== TARGET VARIABLE ===\n",
      "â€¢ Using 'label_num' as target (0=HAM, 1=SPAM)\n",
      "â€¢ Class distribution: {0: 3672, 1: 1499}\n",
      "\n",
      "=== EXAMPLE META-FEATURES (first sample) ===\n",
      "â€¢ subject_has_re: 0.0\n",
      "â€¢ subject_has_fwd: 0.0\n",
      "â€¢ subject_has_currency: 0.0\n",
      "â€¢ subject_digit_count: 6.0\n",
      "â€¢ subject_exclamation_count: 0.0\n",
      "â€¢ subject_length: 33.0\n",
      "â€¢ subject_body_ratio: 0.11827956989247312\n",
      "â€¢ body_phone_count: 0.0\n",
      "â€¢ body_digit_count: 4.0\n",
      "â€¢ body_exclamation_count: 0.0\n",
      "â€¢ body_spam_word_count: 0.0\n",
      "â€¢ body_length: 279.0\n",
      "\n",
      "=== PREPROCESSING DECISIONS ===\n",
      "â€¢ Training data will use deduplication (one instance per group)\n",
      "â€¢ Text preprocessing will:\n",
      "  - Extract meta-features (shown above) before cleaning\n",
      "  - Remove HTML tags and normalize whitespace/case\n",
      "  - Apply PII placeholders: [URL] [EMAIL] [PHONE] [NUMBER]\n",
      "  - Weight subject higher than body (ratio 2:1)\n",
      "  - Truncate content after signatures and 'Original Message' blocks\n",
      "  - Use balanced class weights in the model\n",
      "  - Exclude anti-spam headers that could cause leakage\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 â€” Preprocessing Freeze & Setup\n",
    "\n",
    "# Set key parameters\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "SUBJECT_WEIGHT = 2\n",
    "\n",
    "print(\"=== PREPROCESSING SETUP ===\")\n",
    "print(f\"â€¢ Random Seed: {RANDOM_STATE}\")\n",
    "print(f\"â€¢ Cross-validation folds: {N_FOLDS}\")\n",
    "print(f\"â€¢ Subject weight: {SUBJECT_WEIGHT}x\")\n",
    "\n",
    "# Define target variable\n",
    "print(\"\\n=== TARGET VARIABLE ===\")\n",
    "print(\"â€¢ Using 'label_num' as target (0=HAM, 1=SPAM)\")\n",
    "print(f\"â€¢ Class distribution: {df['label_num'].value_counts().to_dict()}\")\n",
    "\n",
    "# Define meta-features list based on analysis\n",
    "subject_meta_features = [\n",
    "  'subject_has_re', 'subject_has_fwd', 'subject_has_currency',\n",
    "  'subject_digit_count', 'subject_exclamation_count',\n",
    "  'subject_length', 'subject_body_ratio'\n",
    "]\n",
    "body_meta_features = [\n",
    "  'body_phone_count', 'body_digit_count',\n",
    "  'body_exclamation_count', 'body_spam_word_count',\n",
    "  'body_length'\n",
    "]\n",
    "all_meta_features = subject_meta_features + body_meta_features\n",
    "\n",
    "\n",
    "# Create a copy of the dataframe for training with meta-features\n",
    "# Note: Deduplication will be done in the training phase, not here\n",
    "meta_X = df[all_meta_features].copy()\n",
    "y = df['label_num'].copy()\n",
    "\n",
    "# Display example of meta-features for one sample\n",
    "print(\"\\n=== EXAMPLE META-FEATURES (first sample) ===\")\n",
    "example = meta_X.iloc[0].to_dict()\n",
    "for feature, value in example.items():\n",
    "    print(f\"â€¢ {feature}: {value}\")\n",
    "\n",
    "print(\"\\n=== PREPROCESSING DECISIONS ===\")\n",
    "print(\"â€¢ Training data will use deduplication (one instance per group)\")\n",
    "print(\"â€¢ Text preprocessing will:\")\n",
    "print(\"  - Extract meta-features (shown above) before cleaning\")\n",
    "print(\"  - Remove HTML tags and normalize whitespace/case\")\n",
    "print(\"  - Apply PII placeholders: [URL] [EMAIL] [PHONE] [NUMBER]\")\n",
    "print(\"  - Weight subject higher than body (ratio 2:1)\")\n",
    "print(\"  - Truncate content after signatures and 'Original Message' blocks\")\n",
    "print(\"  - Use balanced class weights in the model\")\n",
    "print(\"  - Exclude anti-spam headers that could cause leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e897bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESSING FUNCTIONS ===\n",
      "â€¢ Using pre-calculated meta-features from previous steps\n",
      "\n",
      "Total meta-features: 12\n",
      "\n",
      "â€¢ Creating model-ready DataFrame...\n",
      "\n",
      "â€¢ Creating weighted text (subjectÃ—2):\n",
      "â€¢ Cleaning text with full pipeline\n",
      "WARNING: Found 16 empty text_clean values\n",
      "  â†’ Replaced empty texts with '[EMPTY_TEXT]' placeholder\n",
      "\n",
      "=== EXAMPLE: BEFORE vs AFTER CLEANING ===\n",
      "ORIGINAL:\n",
      "Subject: enron methanol ; meter # : 988291\n",
      "this is a follow up to the note i gave you on monday , 4 / 3 / 00 { preliminary\n",
      "flow data provided by daren } .\n",
      "please override pop ' s daily volume { presently zero } to reflect daily\n",
      "activity you can obtain from gas control .\n",
      "this change is needed asap for economics purposes ....\n",
      "\n",
      "CLEANED:\n",
      "enron methanol ; meter # : [number] enron methanol ; meter # : [number] this is a follow up to the note i gave you on monday , 4 / 3 / 00 { preliminary flow data provided by daren } . please override pop ' s daily volume { presently zero } to reflect daily activity you can obtain from gas control . this change is needed asap for economics purposes ....\n",
      "\n",
      "=== TEXT_CLEAN STATISTICS ===\n",
      "â€¢ Mean length: 1059.93 chars\n",
      "â€¢ Median length: 562.00 chars\n",
      "â€¢ Min length: 4 chars\n",
      "â€¢ Max length: 31964 chars\n",
      "\n",
      "â€¢ Length distribution by class:\n",
      "  - SPAM: 589.00 chars [IQR: 301.50-1273.50]\n",
      "  - HAM: 548.50 chars [IQR: 250.00-1235.25]\n",
      "\n",
      "=== PREPROCESSING COMPLETE ===\n",
      "â€¢ Final DataFrame shape: (5171, 15)\n",
      "â€¢ Features: 12 meta-features + text_clean\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€” Preprocessing functions (correct order)\n",
    "print(\"=== PREPROCESSING FUNCTIONS ===\")\n",
    "\n",
    "# 1. Meta-features first (already calculated in df)\n",
    "print(\"â€¢ Using pre-calculated meta-features from previous steps\")\n",
    "\n",
    "# 2. Text cleaning functions\n",
    "def clean_text(text, remove_leakage=True, truncate=True, replace_pii=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Complete text cleaning pipeline:\n",
    "    - Remove HTML\n",
    "    - Remove anti-spam headers (optional)\n",
    "    - Truncate after signatures and 'Original Message' (optional)\n",
    "    - Replace PII with placeholders (optional)\n",
    "    - Normalize whitespace and lowercase (optional)\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML\n",
    "    if '<' in text and '>' in text:  # Quick check before applying expensive regex\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove anti-spam headers (potential data leakage)\n",
    "    if remove_leakage:\n",
    "        leakage_pattern = r'(?im)^(X-Spam-Flag|Spam-Score|X-Spam-Status|X-Spam-Level|SpamAssassin):.*?$\\n?'\n",
    "        text = re.sub(leakage_pattern, '', text)\n",
    "    \n",
    "    # Truncate after signatures and 'Original Message'\n",
    "    if truncate:\n",
    "        # Signatures (common pattern: --\\n)\n",
    "        signature_match = re.search(r'--\\s*\\n', text)\n",
    "        if signature_match:\n",
    "            text = text[:signature_match.start()].strip()\n",
    "        \n",
    "        # 'Original Message' block\n",
    "        orig_msg_patterns = [\n",
    "            r'(?im)^-{2,}\\s*Original Message\\s*-{2,}',\n",
    "            r'(?ims)^\\s*From: .+\\n\\s*Sent: .+\\n\\s*To: .+\\n\\s*Subject: .+'\n",
    "        ]\n",
    "        \n",
    "        for pattern in orig_msg_patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                text = text[:match.start()].strip()\n",
    "    \n",
    "    # Replace PII with placeholders\n",
    "    if replace_pii:\n",
    "        # URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "        \n",
    "        # Email addresses\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "        \n",
    "        # Phone numbers (various formats)\n",
    "        text = re.sub(r'\\+?\\d[\\d\\s\\-\\(\\)]{7,}\\d', '[PHONE]', text)\n",
    "        \n",
    "        # Number sequences (4+ consecutive digits)\n",
    "        text = re.sub(r'\\b\\d{4,}\\b', '[NUMBER]', text)\n",
    "    \n",
    "    # Normalization\n",
    "    if normalize:\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 3. Function for subject weighting\n",
    "def create_weighted_text(row, subject_weight):\n",
    "    \"\"\"\n",
    "    Create text_weighted = (subject + \" \")*SUBJECT_WEIGHT + body\n",
    "    \"\"\"\n",
    "    subject = str(row['subject']) if pd.notna(row['subject']) else ''\n",
    "    body = str(row['body']) if pd.notna(row['body']) else ''\n",
    "    \n",
    "    # If subject is empty but body isn't, use only body\n",
    "    if subject.strip() == '' and body.strip() != '':\n",
    "        return body\n",
    "    \n",
    "    # If body is empty but subject isn't, use only subject\n",
    "    if body.strip() == '' and subject.strip() != '':\n",
    "        return subject\n",
    "    \n",
    "    # Otherwise apply weighting\n",
    "    return (subject + \" \") * subject_weight + body\n",
    "\n",
    "# 4. Function to create model-ready DataFrame\n",
    "def create_model_ready_df(df, subject_weight, meta_features, deduplicate=True):\n",
    "    \"\"\"\n",
    "    Create model-ready dataframe:\n",
    "    - Add text_weighted and text_clean\n",
    "    - Optionally remove duplicates\n",
    "    - Keep only necessary columns (meta-features + text_clean + label)\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_ready = df.copy()\n",
    "    \n",
    "    # Create text_weighted\n",
    "    print(\"\\nâ€¢ Creating weighted text (subjectÃ—{}):\".format(subject_weight))\n",
    "    df_ready['text_weighted'] = df_ready.apply(\n",
    "        lambda row: create_weighted_text(row, subject_weight), axis=1\n",
    "    )\n",
    "    \n",
    "    # Apply clean_text to text_weighted\n",
    "    print(\"â€¢ Cleaning text with full pipeline\")\n",
    "    df_ready['text_clean'] = df_ready['text_weighted'].apply(clean_text)\n",
    "    \n",
    "    # Remove duplicates (optional)\n",
    "    if deduplicate:\n",
    "        before_count = len(df_ready)\n",
    "        df_ready = df_ready.drop_duplicates('text')\n",
    "        after_count = len(df_ready)\n",
    "        print(f\"â€¢ Removed {before_count - after_count} duplicates ({(before_count - after_count)/before_count*100:.2f}% of data)\")\n",
    "    \n",
    "    # Select columns to keep\n",
    "    keep_cols = meta_features + ['text_clean', 'label_num']\n",
    "    df_ready = df_ready[keep_cols]\n",
    "    \n",
    "    # Check: no NaN/inf in meta-features\n",
    "    nan_meta = df_ready[meta_features].isna().sum().sum()\n",
    "    inf_meta = np.isinf(df_ready[meta_features].values).sum()\n",
    "    \n",
    "    if nan_meta > 0 or inf_meta > 0:\n",
    "        print(f\"WARNING: Found {nan_meta} NaN and {inf_meta} Inf values in meta-features\")\n",
    "        # Fill NaN with 0 and replace Inf with very large but finite values\n",
    "        df_ready[meta_features] = df_ready[meta_features].fillna(0)\n",
    "        df_ready[meta_features] = df_ready[meta_features].replace([np.inf, -np.inf], [1e9, -1e9])\n",
    "        print(\"  â†’ Replaced NaN with 0 and Inf with Â±1e9\")\n",
    "    \n",
    "    # Check: text_clean not empty\n",
    "    empty_texts = (df_ready['text_clean'] == '').sum()\n",
    "    if empty_texts > 0:\n",
    "        print(f\"WARNING: Found {empty_texts} empty text_clean values\")\n",
    "        # Insert placeholder for empty texts\n",
    "        df_ready.loc[df_ready['text_clean'] == '', 'text_clean'] = '[EMPTY_TEXT]'\n",
    "        print(\"  â†’ Replaced empty texts with '[EMPTY_TEXT]' placeholder\")\n",
    "    \n",
    "    return df_ready\n",
    "\n",
    "\n",
    "print(f\"\\nTotal meta-features: {len(all_meta_features)}\")\n",
    "\n",
    "# Create model-ready DataFrame (without deduplication yet)\n",
    "print(\"\\nâ€¢ Creating model-ready DataFrame...\")\n",
    "df_ready = create_model_ready_df(\n",
    "    df, \n",
    "    subject_weight=SUBJECT_WEIGHT,\n",
    "    meta_features=all_meta_features,\n",
    "    deduplicate=False  # Don't deduplicate here, keep both train and test complete\n",
    ")\n",
    "\n",
    "# Show example of text before and after cleaning\n",
    "print(\"\\n=== EXAMPLE: BEFORE vs AFTER CLEANING ===\")\n",
    "sample_idx = 0\n",
    "print(f\"ORIGINAL:\\n{df.iloc[sample_idx]['text'][:500]}...\")\n",
    "print(f\"\\nCLEANED:\\n{df_ready.iloc[sample_idx]['text_clean'][:500]}...\")\n",
    "\n",
    "# Statistics on text_clean\n",
    "print(\"\\n=== TEXT_CLEAN STATISTICS ===\")\n",
    "df_ready['clean_length'] = df_ready['text_clean'].str.len()\n",
    "print(f\"â€¢ Mean length: {df_ready['clean_length'].mean():.2f} chars\")\n",
    "print(f\"â€¢ Median length: {df_ready['clean_length'].median():.2f} chars\")\n",
    "print(f\"â€¢ Min length: {df_ready['clean_length'].min()} chars\")\n",
    "print(f\"â€¢ Max length: {df_ready['clean_length'].max()} chars\")\n",
    "\n",
    "# Report length distribution by class\n",
    "print(\"\\nâ€¢ Length distribution by class:\")\n",
    "for label, name in [(1, \"SPAM\"), (0, \"HAM\")]:\n",
    "    subset = df_ready[df_ready['label_num'] == label]['clean_length']\n",
    "    print(f\"  - {name}: {subset.median():.2f} chars [IQR: {subset.quantile(0.25):.2f}-{subset.quantile(0.75):.2f}]\")\n",
    "\n",
    "print(\"\\n=== PREPROCESSING COMPLETE ===\")\n",
    "print(f\"â€¢ Final DataFrame shape: {df_ready.shape}\")\n",
    "print(f\"â€¢ Features: {len(all_meta_features)} meta-features + text_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "780f5945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE REPRESENTATION SETUP ===\n",
      "\n",
      "â€¢ Text Representation:\n",
      "  - Using TF-IDF Vectorizer with 1-2 grams\n",
      "  - Minimum document frequency: 2\n",
      "  - Sublinear TF scaling: Yes\n",
      "  - English stopwords: Yes\n",
      "  - Max features: 150,000\n",
      "\n",
      "=== META-FEATURES INCLUDED ===\n",
      "Subject-related:\n",
      "  - subject_has_re\n",
      "  - subject_has_fwd\n",
      "  - subject_has_currency\n",
      "  - subject_digit_count\n",
      "  - subject_exclamation_count\n",
      "  - subject_length\n",
      "  - subject_body_ratio\n",
      "\n",
      "Body-related:\n",
      "  - body_phone_count\n",
      "  - body_digit_count\n",
      "  - body_exclamation_count\n",
      "  - body_spam_word_count\n",
      "  - body_length\n",
      "\n",
      "â€¢ Creating weighted text (subjectÃ—2):\n",
      "â€¢ Cleaning text with full pipeline\n",
      "â€¢ Removed 178 duplicates (3.44% of data)\n",
      "WARNING: Found 1 empty text_clean values\n",
      "  â†’ Replaced empty texts with '[EMPTY_TEXT]' placeholder\n",
      "df_ready_model shape: (4993, 14)\n",
      "\n",
      "Estimated vocabulary size (TF-IDF params): 71,878 n-grams\n",
      "Limited to max_features: 71,878 n-grams\n",
      "Total feature dimensionality (approx): 71,890\n",
      "Deduplicated rows: 178 (3.44%)\n",
      "\n",
      "=== FEATURE REPRESENTATION COMPLETE ===\n",
      "Templates & factory ready. Using df_ready_model for training.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” Feature Representation Definition\n",
    "print(\"=== FEATURE REPRESENTATION SETUP ===\")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer as FT\n",
    "\n",
    "# 1. TF-IDF for text features\n",
    "print(\"\\nâ€¢ Text Representation:\")\n",
    "print(\"  - Using TF-IDF Vectorizer with 1-2 grams\")\n",
    "print(\"  - Minimum document frequency: 2\")\n",
    "print(\"  - Sublinear TF scaling: Yes\")\n",
    "print(\"  - English stopwords: Yes\")\n",
    "print(\"  - Max features: 150,000\")\n",
    "\n",
    "# --- Template TF-IDF (word) ---\n",
    "TFIDF_WORD_TEMPLATE = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    max_features=150_000,\n",
    "    stop_words='english',\n",
    "    strip_accents='unicode',\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "# --- Template TF-IDF (char) ---\n",
    "TFIDF_CHAR_TEMPLATE = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    max_features=50_000,\n",
    "    strip_accents='unicode',\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "# --- Ramo numerico: SPARSE -> SCALER (resta sparse end-to-end) ---\n",
    "def build_num_pipeline():\n",
    "    return Pipeline([\n",
    "        ('to_sparse', FunctionTransformer(lambda X: csr_matrix(X), validate=False)),\n",
    "        ('scaler', StandardScaler(with_mean=False))\n",
    "    ])\n",
    "\n",
    "# --- Selettore 1D del testo per sicurezza ---\n",
    "text_selector = FT(lambda X: X['text_clean'], validate=False)\n",
    "\n",
    "# --- Factory per costruire il ColumnTransformer ---\n",
    "def build_features_transformer(all_meta_features, mode='word'):\n",
    "    num_pipeline = build_num_pipeline()\n",
    "    if mode == 'word':\n",
    "        text_branch = Pipeline([\n",
    "            ('select_text', text_selector),\n",
    "            ('tfidf', clone(TFIDF_WORD_TEMPLATE))\n",
    "        ])\n",
    "    elif mode == 'word_char':\n",
    "        text_branch = Pipeline([\n",
    "            ('select_text', text_selector),\n",
    "            ('union', FeatureUnion([\n",
    "                ('word', clone(TFIDF_WORD_TEMPLATE)),\n",
    "                ('char', clone(TFIDF_CHAR_TEMPLATE))\n",
    "            ]))\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'word' or 'word_char'\")\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text_features', text_branch, ['text_clean']),\n",
    "            ('meta_features', num_pipeline, all_meta_features)\n",
    "        ],\n",
    "        remainder='drop',\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Stima coerente del vocabolario (solo info di servizio) ---\n",
    "def estimate_vocab_size(texts):\n",
    "    probe = clone(TFIDF_WORD_TEMPLATE)\n",
    "    probe.fit(texts)\n",
    "    return len(probe.vocabulary_)\n",
    "\n",
    "# Print information about the meta-features\n",
    "print(\"\\n=== META-FEATURES INCLUDED ===\")\n",
    "print(\"Subject-related:\")\n",
    "for feature in [f for f in all_meta_features if f.startswith('subject_')]:\n",
    "    print(f\"  - {feature}\")\n",
    "print(\"\\nBody-related:\")\n",
    "for feature in [f for f in all_meta_features if f.startswith('body_')]:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Create model-ready DataFrame without \n",
    "df_ready_model = create_model_ready_df(\n",
    "    df,\n",
    "    subject_weight=SUBJECT_WEIGHT,\n",
    "    meta_features=all_meta_features,\n",
    "    deduplicate=True \n",
    ")\n",
    "\n",
    "print(\"df_ready_model shape:\", df_ready_model.shape)\n",
    "\n",
    "# Estimate vocabulary size\n",
    "vocab_size = estimate_vocab_size(df_ready_model['text_clean'])\n",
    "print(f\"\\nEstimated vocabulary size (TF-IDF params): {vocab_size:,} n-grams\")\n",
    "print(f\"Limited to max_features: {min(vocab_size, TFIDF_WORD_TEMPLATE.max_features):,} n-grams\")\n",
    "\n",
    "feature_dim = min(vocab_size, TFIDF_WORD_TEMPLATE.max_features or vocab_size) + len(all_meta_features)\n",
    "print(f\"Total feature dimensionality (approx): {feature_dim:,}\")\n",
    "print(f\"Deduplicated rows: {len(df) - len(df_ready_model)} \"\n",
    "      f\"({(len(df) - len(df_ready_model)) / len(df) * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\n=== FEATURE REPRESENTATION COMPLETE ===\")\n",
    "print(\"Templates & factory ready. Using df_ready_model for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f95aaf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE MODEL SETUP ===\n",
      "A sparse? True (1000, 13850)\n",
      "B sparse? True (1000, 63850)\n",
      "\n",
      "=== BASELINE MODEL READY ===\n",
      "Default: Version A (Word n-grams); Version B per confronto.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BASELINE MODEL SETUP ===\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "baseline_model = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    "    solver=\"liblinear\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Riusa la factory della Cella 2\n",
    "features_transformer_A = build_features_transformer(all_meta_features, mode='word')\n",
    "features_transformer_B = build_features_transformer(all_meta_features, mode='word_char')\n",
    "\n",
    "pipeline_A = Pipeline([('features', features_transformer_A),\n",
    "                       ('classifier', baseline_model)])\n",
    "pipeline_B = Pipeline([('features', features_transformer_B),\n",
    "                       ('classifier', baseline_model)])\n",
    "\n",
    "# Test rapido memoria/densitÃ  su un campione\n",
    "sample_size = min(1000, len(df_ready_model))\n",
    "X_sample = df_ready_model.iloc[:sample_size]\n",
    "XA = features_transformer_A.fit_transform(X_sample)\n",
    "XB = features_transformer_B.fit_transform(X_sample)\n",
    "print(\"A sparse?\", issparse(XA), XA.shape)\n",
    "print(\"B sparse?\", issparse(XB), XB.shape)\n",
    "\n",
    "print(\"\\n=== BASELINE MODEL READY ===\")\n",
    "print(\"Default: Version A (Word n-grams); Version B per confronto.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1876f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 â€” Cross-validation + metrics\n",
    "\n",
    "print(\"=== CROSS-VALIDATION & METRICS ===\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, f1_score,\n",
    "    precision_recall_curve, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Stratified 80/20 split ---\n",
    "print(\"\\nâ€¢ Stratified 80/20 split (holdout test set)\")\n",
    "X = df_ready_model.drop(columns=['label_num'])\n",
    "y = df_ready_model['label_num'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Spam % in train: {np.mean(y_train)*100:.2f}%, test: {np.mean(y_test)*100:.2f}%\")\n",
    "\n",
    "# --- 2. CV setup ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 3. CV loop ---\n",
    "pr_aucs, roc_aucs, f1s_05 = [], [], []\n",
    "f1_max_thresholds, highrec_thresholds = [], []\n",
    "oof_pred = np.zeros(len(X_train))\n",
    "oof_true = np.zeros(len(X_train))\n",
    "fold_indices = np.zeros(len(X_train), dtype=int)\n",
    "\n",
    "print(\"\\nâ€¢ Starting cross-validation (pipeline_A)...\")\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "\n",
    "    # Fit pipeline\n",
    "    pipeline_A.fit(X_tr, y_tr)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_val_proba = pipeline_A.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Save OOF predictions\n",
    "    oof_pred[val_idx] = y_val_proba\n",
    "    oof_true[val_idx] = y_val\n",
    "    fold_indices[val_idx] = fold\n",
    "\n",
    "    # Metrics\n",
    "    pr_auc = average_precision_score(y_val, y_val_proba)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    y_val_pred_05 = (y_val_proba >= 0.5).astype(int)\n",
    "    f1_05 = f1_score(y_val, y_val_pred_05, pos_label=1)\n",
    "\n",
    "    pr_aucs.append(pr_auc)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    f1s_05.append(f1_05)\n",
    "               # <-- niente -1\n",
    "\n",
    "    # Threshold tuning\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "    prec_t, rec_t = precisions[:-1], recalls[:-1]\n",
    "    f1_scores = 2 * (prec_t * rec_t) / (prec_t + rec_t + 1e-8)\n",
    "    best_idx = int(np.nanargmax(f1_scores))\n",
    "    f1_max_thr = thresholds[best_idx]\n",
    "    f1_max_thresholds.append(f1_max_thr)\n",
    "\n",
    "    # High-recall threshold (Recall >= 0.95)\n",
    "    highrec_thr = None\n",
    "    mask = rec_t >= 0.95\n",
    "    if np.any(mask):  # exclude last point (always recall=1)\n",
    "        highrec_thr = thresholds[np.where(mask)[0][-1]]\n",
    "    else:\n",
    "        highrec_thr = thresholds[0]  # fallback: lowest threshold\n",
    "    highrec_thresholds.append(highrec_thr)\n",
    "\n",
    "    print(f\"PR-AUC: {pr_auc:.4f} | ROC-AUC: {roc_auc:.4f} | F1@0.5: {f1_05:.4f}\")\n",
    "    print(f\"Best F1 threshold: {f1_max_thr:.4f} | High-recall (recallâ‰¥0.95) threshold: {highrec_thr:.4f}\")\n",
    "\n",
    "# --- 4. Aggregate metrics report ---\n",
    "def mean_std(arr):\n",
    "    return f\"{np.mean(arr):.4f} Â± {np.std(arr):.4f}\"\n",
    "\n",
    "print(\"\\n=== CV RESULTS (mean Â± std) ===\")\n",
    "print(f\"PR-AUC:    {mean_std(pr_aucs)}\")\n",
    "print(f\"ROC-AUC:   {mean_std(roc_aucs)}\")\n",
    "print(f\"F1@0.5:    {mean_std(f1s_05)}\")\n",
    "print(f\"F1-max threshold (mean):      {np.mean(f1_max_thresholds):.4f}\")\n",
    "print(f\"High-recall threshold (mean): {np.mean(highrec_thresholds):.4f}\")\n",
    "\n",
    "# --- 5. Aggregated OOF confusion matrix ---\n",
    "# Chosen threshold: mean of F1-max thresholds\n",
    "chosen_thr = np.mean(f1_max_thresholds)\n",
    "oof_pred_label = (oof_pred >= chosen_thr).astype(int)\n",
    "cm = confusion_matrix(oof_true, oof_pred_label)\n",
    "cm_df = pd.DataFrame(cm, index=[\"HAM (true)\", \"SPAM (true)\"], columns=[\"HAM (pred)\", \"SPAM (pred)\"])\n",
    "\n",
    "print(f\"\\n=== OOF CONFUSION MATRIX (mean F1-max threshold = {chosen_thr:.4f}) ===\")\n",
    "print(cm_df)\n",
    "\n",
    "# Global OOF metrics\n",
    "oof_pr_auc  = average_precision_score(oof_true, oof_pred)\n",
    "oof_roc_auc = roc_auc_score(oof_true, oof_pred)\n",
    "print(f\"\\n=== OOF METRICS (global) ===\")\n",
    "print(f\"PR-AUC (OOF):  {oof_pr_auc:.4f}\")\n",
    "print(f\"ROC-AUC (OOF): {oof_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Save results for later use ---\n",
    "cv_results = {\n",
    "    \"pr_aucs\": pr_aucs,\n",
    "    \"roc_aucs\": roc_aucs,\n",
    "    \"f1s_05\": f1s_05,\n",
    "    \"f1_max_thresholds\": f1_max_thresholds,\n",
    "    \"highrec_thresholds\": highrec_thresholds,\n",
    "    \"oof_pred\": oof_pred,\n",
    "    \"oof_true\": oof_true,\n",
    "    \"fold_indices\": fold_indices,\n",
    "    \"chosen_thr\": chosen_thr,\n",
    "    \"confusion_matrix\": cm,\n",
    "    \"oof_pr_auc\": oof_pr_auc,\n",
    "    \"oof_roc_auc\": oof_roc_auc\n",
    "}\n",
    "print(\"\\n=== CV COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
